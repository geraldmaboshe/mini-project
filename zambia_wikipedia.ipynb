{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df32a868-f983-4f73-b0c0-1bd248d05811",
   "metadata": {},
   "source": [
    "### Mini Project Codebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "06005872-b1cf-4345-a92d-2d16b043a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0ae837e1-358e-4541-8fbd-8bdb5371c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Zambian-related Wikipedia pages\n",
    "pages = [\n",
    "    \"History_of_Zambia\",\n",
    "    \"Economy_of_Zambia\",\n",
    "    \"University_Teaching_Hospital\",\n",
    "    \"Lusaka\",\n",
    "    \"University_of_Zambia\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dcf0bae8-39be-4211-9641-6a036522db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch and parse Wikipedia page\n",
    "def get_page_content(page_title):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{page_title}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {page_title}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a178cc62-7868-4b39-b2a7-a2bb2a687190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract section features\n",
    "def extract_section_features(section, section_level):\n",
    "    text = section.get_text().strip()\n",
    "    word_count = len(re.findall(r'\\w+', text))\n",
    "    references = len(section.find_all('sup', class_='reference'))\n",
    "    image_count = len(section.find_all('img'))\n",
    "    return {\n",
    "        'Word_Count': word_count,\n",
    "        'References': references,\n",
    "        'Images': image_count,\n",
    "        'Section_Depth': section_level\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "01daae39-e5cf-4f71-bcec-7f4d5e6420d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign ground-truth labels\n",
    "def assign_completeness_label(word_count, references):\n",
    "    if word_count > 100 and references >= 1:\n",
    "        return \"Complete\"\n",
    "    elif word_count > 50 or references == 1:\n",
    "        return \"Partial\"\n",
    "    else:\n",
    "        return \"Incomplete\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0c2db9ed-c06c-4bb4-a41e-754bea79fdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Multi-party_democracy has no content, skipping\n",
      "Section History has no content, skipping\n",
      "Section Sectors has no content, skipping\n"
     ]
    }
   ],
   "source": [
    "# Initialize data storage\n",
    "data = []\n",
    "\n",
    "# Collect and preprocess data\n",
    "for page in pages:\n",
    "    soup = get_page_content(page)\n",
    "    if not soup:\n",
    "        print(f\"Skipping {page} due to fetch error\")\n",
    "        continue\n",
    "    \n",
    "# Find all heading divs (mw-heading for h2 and h3)\n",
    "    heading_divs = soup.find_all('div', class_=re.compile('mw-heading mw-heading[2-3]'))\n",
    "    \n",
    "    for i, heading_div in enumerate(heading_divs):\n",
    "        # Extract title from h2 or h3 tag's id attribute\n",
    "        section = heading_div.find(['h2', 'h3'])\n",
    "        if not section:\n",
    "            print(\"Skipping section with no h2/h3 tag\")\n",
    "            continue\n",
    "        title = section.get('id') or section.get_text().strip()\n",
    "        if not title or title in ['mw-hidden-catlinks', 'mw-references']:\n",
    "            print(f\"Skipping section with invalid title: {title}\")\n",
    "            continue\n",
    "        section_title = re.sub(r'\\[edit\\]', '', title).strip()\n",
    "        \n",
    "        # Determine section depth (h2=1, h3=2)\n",
    "        section_level = 1 if section.name == 'h2' else 2\n",
    "        \n",
    "        # Collect content until the next heading div or end of page\n",
    "        content = []\n",
    "        current = heading_div.next_sibling\n",
    "        while current and not (current.name == 'div' and 'mw-heading' in current.get('class', [])):\n",
    "            if current.name and current.name not in ['script', 'style']:  # Exclude scripts/styles\n",
    "                content.append(current)\n",
    "            current = current.next_sibling if current else None\n",
    "        \n",
    "        if not content:\n",
    "            print(f\"Section {section_title} has no content, skipping\")\n",
    "            continue\n",
    "        \n",
    "        section_soup = BeautifulSoup(''.join(str(c) for c in content), 'html.parser')\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_section_features(section_soup, section_level)\n",
    "        \n",
    "        # Assign ground-truth label\n",
    "        completeness = assign_completeness_label(features['Word_Count'], features['References'])\n",
    "        \n",
    "        # Store data\n",
    "        data.append({\n",
    "            'Page': page,\n",
    "            'Section': section_title,\n",
    "            'Word_Count': features['Word_Count'],\n",
    "            'References': features['References'],\n",
    "            'Images': features['Images'],\n",
    "            'Section_Depth': features['Section_Depth'],\n",
    "            'Completeness': completeness\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "ab1a1abb-95a1-4c36-9bcc-f8d081685040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 92 sections\n",
      "                Page                 Section  Word_Count  References  Images  \\\n",
      "0  History_of_Zambia         Prehistoric_era          65           1       0   \n",
      "1  History_of_Zambia       Khoisan_and_Batwa         153           2       1   \n",
      "2  History_of_Zambia          Bantu_(Abantu)         138           1       1   \n",
      "3  History_of_Zambia           Bantu_origins          96           1       0   \n",
      "4  History_of_Zambia  First_Bantu_settlement         585           4       5   \n",
      "\n",
      "   Section_Depth Completeness  \n",
      "0              1      Partial  \n",
      "1              1     Complete  \n",
      "2              1     Complete  \n",
      "3              2      Partial  \n",
      "4              2     Complete  \n",
      "\n",
      "Raw data saved to 'raw_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Verify data\n",
    "if df.empty:\n",
    "    print(\"\\nError: No data collected. Check network connection or Wikipedia page structure.\")\n",
    "else:\n",
    "    print(f\"\\nCollected {len(df)} sections\")\n",
    "    print(df.head())\n",
    "\n",
    "# Save raw data\n",
    "df.to_csv('raw_data.csv', index=False)\n",
    "print(\"\\nRaw data saved to 'raw_data.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4b9e471a-014d-42d5-bc9f-08ddcb0f6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Complete       1.00      1.00      1.00        12\n",
      "  Incomplete       0.67      1.00      0.80         2\n",
      "     Partial       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.95        19\n",
      "   macro avg       0.89      0.93      0.90        19\n",
      "weighted avg       0.96      0.95      0.95        19\n",
      "\n",
      "\n",
      "Processed Data:\n",
      "                    Page                 Section  Word_Count  References  \\\n",
      "0      History_of_Zambia         Prehistoric_era          65           1   \n",
      "1      History_of_Zambia       Khoisan_and_Batwa         153           2   \n",
      "2      History_of_Zambia          Bantu_(Abantu)         138           1   \n",
      "3      History_of_Zambia           Bantu_origins          96           1   \n",
      "4      History_of_Zambia  First_Bantu_settlement         585           4   \n",
      "..                   ...                     ...         ...         ...   \n",
      "87  University_of_Zambia            Affiliations          23           0   \n",
      "88  University_of_Zambia          Notable_people        1402           0   \n",
      "89  University_of_Zambia                See_also           7           0   \n",
      "90  University_of_Zambia              References         307           0   \n",
      "91  University_of_Zambia          External_links        1014           0   \n",
      "\n",
      "   Completeness Predicted_Completeness  \n",
      "0       Partial                Partial  \n",
      "1      Complete               Complete  \n",
      "2      Complete               Complete  \n",
      "3       Partial                Partial  \n",
      "4      Complete               Complete  \n",
      "..          ...                    ...  \n",
      "87   Incomplete             Incomplete  \n",
      "88      Partial                Partial  \n",
      "89   Incomplete             Incomplete  \n",
      "90      Partial                Partial  \n",
      "91      Partial                Partial  \n",
      "\n",
      "[92 rows x 6 columns]\n",
      "\n",
      "Processed data exported to 'zambian_wikipedia_completeness.csv'\n",
      "Preprocessed features saved to 'preprocessed_features.csv'\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for classification\n",
    "if not df.empty:\n",
    "    X = df[['Word_Count', 'References', 'Images', 'Section_Depth']]\n",
    "    y = df['Completeness']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train Decision Tree Classifier\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Predict completeness for all data\n",
    "    df['Predicted_Completeness'] = clf.predict(X)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nProcessed Data:\")\n",
    "    print(df[['Page', 'Section', 'Word_Count', 'References', 'Completeness', 'Predicted_Completeness']])\n",
    "\n",
    "    # Export processed data\n",
    "    df.to_csv('zambian_wikipedia_completeness.csv', index=False)\n",
    "    print(\"\\nProcessed data exported to 'zambian_wikipedia_completeness.csv'\")\n",
    "\n",
    "    # Save preprocessed features\n",
    "    X.to_csv('preprocessed_features.csv', index=False)\n",
    "    print(\"Preprocessed features saved to 'preprocessed_features.csv'\")\n",
    "else:\n",
    "    print(\"\\nSkipping classification due to empty dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306c937-6208-4fbb-8adc-06668fa536cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4575b69-3e22-4c83-a809-ca607b2974f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d2680-2373-4e95-b9b3-c18a3cb4467c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6ad71-e516-4766-8cb5-2f0ef9184391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
